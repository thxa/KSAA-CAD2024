{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "IK6eIq55y6"
   },
   "source": [
    "This notebook just for try to train in dev data before deep dive in training data\n",
    "\n",
    "this is important:\n",
    "https://groups.google.com/g/sigarab/c/8e3Os4Q0Aac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "GTXhZxzpWj"
   },
   "source": [
    "# Trying to learn how to do NLP and LLms Predictions\n",
    "ref :\n",
    "https://towardsdatascience.com/machine-learning-word-embedding-and-predicting-e603254e4d7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "6ryRZYIdSK"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "w4Av87YncE"
   },
   "outputs": [],
   "source": [
    "with open('dev.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jukit_cell_id": "oFMDTsr7rA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3921 entries, 0 to 3920\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       3921 non-null   object\n",
      " 1   word     3921 non-null   object\n",
      " 2   pos      3636 non-null   object\n",
      " 3   gloss    3921 non-null   object\n",
      " 4   electra  3921 non-null   object\n",
      " 5   bertseg  3921 non-null   object\n",
      " 6   bertmsa  3921 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 214.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThis is base frameworks used:\\n - Electra (Clark et al., 2020) \\n - BERT (Devlin et al., 2019),\\n\\n\\nin our dataframe Embedding:\\n- AraELECTRA (Antoun et al., 2021)    -> electra \\n- AraBERTv2 (Antoun et al., 2020)     ->  bertseg\\n- camelBERT-MSA (Inoue et al., 2021)  -> bertmsa\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.info()\n",
    "\n",
    "\"\"\"\n",
    "This is base frameworks used:\n",
    " - Electra (Clark et al., 2020) \n",
    " - BERT (Devlin et al., 2019),\n",
    "\n",
    "\n",
    "in our dataframe Embedding:\n",
    "- AraELECTRA (Antoun et al., 2021)    -> electra \n",
    "- AraBERTv2 (Antoun et al., 2020)     ->  bertseg\n",
    "- camelBERT-MSA (Inoue et al., 2021)  -> bertmsa\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jukit_cell_id": "9uU4LdoJ4X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar.962714 أَكْمَدَ V غَمَّ وَأَمْرَضَ القَلْبَ\n",
      "ar.994971 ب None حرف جرّ زائد في التوكيد بالنفس والعين .\n",
      "ar.989034 ذَكَا V ذَكَا الشَّاةَ ونحوَها: ذَبَحَها\n",
      "ar.994539 وَرَع V وَرَعَ فلانٌ: جَبُنَ.\n",
      "ar.992756 قَلَص V قَلَصَ الظِّلُّ عنِّي: انْقَبَض ونَقَصَ.\n",
      "ar.993508 أَمْرَضَ V أَمْرَضَ فلانٌ: قَارَبَ الصَّوابَ في الرَّأْي وإِن لم يُصِبْ كلَّ الصَّواب.\n",
      "ar.977791 عليق N : ما يُقدَّم للدَّابَّة من طعام \"عليقٌ من شعير/ حشيش- عليقٌ يابس\".\n",
      "ar.977267 تطوع V تبرَّع به، وزاد على ما يجب عليه\n",
      "ar.968835 قَصْر اَلرِّئَاسَةِ None مقرّ إقامة رئيس الدولة.\n",
      "ar.978831 مقدم N من يلْتَمس شيئًا .\n"
     ]
    }
   ],
   "source": [
    "for i in dev[:10]:\n",
    "    print(i['id'], i['word'], i[\"pos\"], i[\"gloss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "E1iPd283Fm"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example input\n",
    "word = \"example\"\n",
    "pos = \"noun\"\n",
    "gloss = \"a thing characteristic of its kind or illustrating a general rule.\"\n",
    "\n",
    "# Tokenize inputs\n",
    "word_tokens = tokenizer.encode(word, add_special_tokens=False, max_length=512, truncation=True)\n",
    "pos_token = tokenizer.encode(pos, add_special_tokens=False, max_length=512, truncation=True)\n",
    "gloss_tokens = tokenizer.encode(gloss, add_special_tokens=False, max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jukit_cell_id": "8HlX8L7bCg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2742, 102, 15156, 102, 1037, 2518, 8281, 1997, 2049, 2785, 2030, 28252, 1037, 2236, 3627, 1012, 102]\n",
      "tf.Tensor(\n",
      "[  101  2742   102 15156   102  1037  2518  8281  1997  2049  2785  2030\n",
      " 28252  1037  2236  3627  1012   102], shape=(18,), dtype=int32)\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[62], line 11\u001b[0m\n",
      "\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_ids)\n",
      "\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Get BERT embeddings\u001b[39;00m\n",
      "\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# word_embedding = outputs[0][:, 0, :]  # Assuming you want to use the [CLS] token as the word embedding\u001b[39;00m\n",
      "\u001b[1;32m     13\u001b[0m \n",
      "\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Further process or concatenate embeddings as needed\u001b[39;00m\n",
      "\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# ...\u001b[39;00m\n",
      "\u001b[1;32m     16\u001b[0m \n",
      "\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Now you can use word_embedding as the input to your model\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n",
      "\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n",
      "\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:426\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    423\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n",
      "\u001b[1;32m    425\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n",
      "\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/bert/modeling_tf_bert.py:1088\u001b[0m, in \u001b[0;36mTFBertModel.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n",
      "\u001b[1;32m   1044\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n",
      "\u001b[1;32m   1045\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(BERT_INPUTS_DOCSTRING\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size, sequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;32m   1046\u001b[0m \u001b[38;5;129m@add_code_sample_docstrings\u001b[39m(\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1066\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "\u001b[1;32m   1067\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TFBaseModelOutputWithPoolingAndCrossAttentions, Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]]:\n",
      "\u001b[1;32m   1068\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\u001b[39;00m\n",
      "\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1086\u001b[0m \u001b[38;5;124;03m        `past_key_values`). Set to `False` during training, `True` during generation\u001b[39;00m\n",
      "\u001b[1;32m   1087\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m-> 1088\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1089\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1100\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:426\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    423\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n",
      "\u001b[1;32m    425\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n",
      "\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/bert/modeling_tf_bert.py:766\u001b[0m, in \u001b[0;36mTFBertMainLayer.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n",
      "\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m--> 766\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n",
      "\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    769\u001b[0m     past_key_values_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'bert' (type TFBertMainLayer).\n",
      "\n",
      "not enough values to unpack (expected 2, got 1)\n",
      "\n",
      "Call arguments received by layer 'bert' (type TFBertMainLayer):\n",
      "  • input_ids=tf.Tensor(shape=(18,), dtype=int32)\n",
      "  • attention_mask=None\n",
      "  • token_type_ids=None\n",
      "  • position_ids=None\n",
      "  • head_mask=None\n",
      "  • inputs_embeds=None\n",
      "  • encoder_hidden_states=None\n",
      "  • encoder_attention_mask=None\n",
      "  • past_key_values=None\n",
      "  • use_cache=True\n",
      "  • output_attentions=False\n",
      "  • output_hidden_states=False\n",
      "  • return_dict=True\n",
      "  • training=False\n"
     ]
    }
   ],
   "source": [
    "# Combine token IDs and add special tokens\n",
    "# input_ids = tokenizer.build_inputs_with_special_tokens(word_tokens, pos_token, gloss_tokens)\n",
    "input_ids = [tokenizer.cls_token_id] + word_tokens + [tokenizer.sep_token_id] + pos_token + [tokenizer.sep_token_id] + gloss_tokens + [tokenizer.sep_token_id]\n",
    "\n",
    "print(input_ids)\n",
    "# Convert to tensor\n",
    "input_ids = tf.convert_to_tensor(input_ids)\n",
    "\n",
    "print(input_ids)\n",
    "# Get BERT embeddings\n",
    "outputs = bert_model(input_ids)\n",
    "# word_embedding = outputs[0][:, 0, :]  # Assuming you want to use the [CLS] token as the word embedding\n",
    "\n",
    "# Further process or concatenate embeddings as needed\n",
    "# ...\n",
    "\n",
    "# Now you can use word_embedding as the input to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jukit_cell_id": "quHythKADp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m\n",
      "\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# # Convert text to sequences\u001b[39;00m\n",
      "\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# X = tokenizer.texts_to_sequences(glosses)\u001b[39;00m\n",
      "\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# y = tokenizer.texts_to_sequences(words)\u001b[39;00m\n",
      "\u001b[1;32m     15\u001b[0m \n",
      "\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Pad sequences to ensure uniform length\u001b[39;00m\n",
      "\u001b[1;32m     17\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m X)\n",
      "\u001b[0;32m---> 18\u001b[0m X_padded \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     19\u001b[0m y_padded \u001b[38;5;241m=\u001b[39m pad_sequences(y, maxlen\u001b[38;5;241m=\u001b[39mmax_len)\n",
      "\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Prepare the data for training\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/data_utils.py:1132\u001b[0m, in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n",
      "\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTruncating type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtruncating\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not understood\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# check `trunc` has expected shape\u001b[39;00m\n",
      "\u001b[0;32m-> 1132\u001b[0m trunc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(trunc, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trunc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m!=\u001b[39m sample_shape:\n",
      "\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[1;32m   1135\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrunc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of sequence at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   1136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is different from expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   1137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   1138\u001b[0m     )\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'word'\n"
     ]
    }
   ],
   "source": [
    "X = df[['word', 'pos', 'gloss']]\n",
    "y = df[['electra', 'bertseg', 'bertmsa']]\n",
    "\n",
    "# Preprocess the data if necessary\n",
    "# For example, you might need to handle missing values or tokenize the text\n",
    "\n",
    "\n",
    "# Tokenize the glosses and words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X[\"gloss\"])\n",
    "\n",
    "# # Convert text to sequences\n",
    "# X = tokenizer.texts_to_sequences(glosses)\n",
    "# y = tokenizer.texts_to_sequences(words)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_len = max(len(seq) for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_len)\n",
    "y_padded = pad_sequences(y, maxlen=max_len)\n",
    "\n",
    "# Prepare the data for training\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_padded, y_padded, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Model Building\n",
    "vocab_size = 10000  # Define your vocabulary size\n",
    "embedding_dim = 50  # Define the dimensionality of your word embeddings\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=3),  # Assuming 3 input features\n",
    "    LSTM(units=64),\n",
    "    Dense(units=3)  # Output dimension should match the number of output columns (3 in this case)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')  # Use mean squared error as the loss function\n",
    "\n",
    "model.summary()\n",
    "# Step 3: Training\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "# Step 4: Evaluation\n",
    "loss = model.evaluate(X_val, y_val)\n",
    "\n",
    "# Step 5: Inference\n",
    "# Use the trained model to generate output predictions for new input data\n",
    "\n",
    "# Step 6: Model Deployment\n",
    "# Deploy the trained model if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "aLKDj2tFNh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
