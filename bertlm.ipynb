{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "7HRfWZ6g5s"
      },
      "source": [],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Tokenized text: ['the', 'cat', '[MASK]', 'on', 'the', 'mat', '.']\nInput IDs: [1996, 4937, 103, 2006, 1996, 13523, 1012]\nPredicted token IDs: [1012, 1012, 1012, 1012, 1012, 1012, 1012]\nPredicted tokens: ['.', '.', '.', '.', '.', '.', '.']\n"
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "ZePsjabb0Y"
      },
      "source": [
        "import torch\n",
        "# from torch import  Tensor\n",
        "from torch.nn import attention\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForMaskedLM, AdamW"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "DWnDxCTnBL"
      },
      "source": [
        "# Create dataset and dataloader\n",
        "data = [\n",
        "        \"The quick brown fox jumps over the lazy dog.\",\n",
        "        \"She sells seashells by the seashore.\",\n",
        "        \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\",\n",
        "        \"The quick brown fox jumps over the lazy dog.\",\n",
        "        \"She sells seashells by the seashore.\",\n",
        "        \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\",\n",
        "        \"The cat in the hat\",\n",
        "        \"To be or not to be, that is the question.\",\n",
        "        \"A journey of a thousand miles begins with a single step.\",\n",
        "        \"All's well that ends well.\",\n",
        "        \"Actions speak louder than words.\",\n",
        "        \"Beauty is in the eye of the beholder.\",\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "dpJ8TYYvgR"
      },
      "source": [
        "# Define a custom dataset for masked language model training\n",
        "\n",
        "def fit_x(data, max_length=128, pad_token_id=0):\n",
        "    if len(data) < max_length:\n",
        "        data=data+[pad_token_id]*(max_length-len(data))\n",
        "    else:\n",
        "        data=data[:max_length]\n",
        "    return data\n",
        "\n",
        "\n",
        "def pad_data(data, pad_token_id=0, max_length=128):\n",
        "    input_ids = fit_x(data[\"input_ids\"], pad_token_id=pad_token_id, max_length=128)\n",
        "    token_type_ids = fit_x(data[\"token_type_ids\"], pad_token_id=pad_token_id, max_length=128)\n",
        "    attention_mask = fit_x(data[\"attention_mask\"], pad_token_id=pad_token_id, max_length=128)\n",
        "\n",
        "    input_ids=torch.tensor([input_ids])\n",
        "    token_type_ids=torch.tensor([token_type_ids])\n",
        "    attention_mask=torch.tensor([attention_mask])\n",
        "\n",
        "    return input_ids, token_type_ids, attention_mask\n",
        "\n",
        "\n",
        "\n",
        "    return input_ids, token_type_ids, attention_mask\n",
        "\n",
        "class MaskedLMDataset(Dataset):\n",
        "    MAX_LEN = 128\n",
        "\n",
        "    def __init__(self, tokenizer, data):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx]\n",
        "        # inputs = self.tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt', truncation=True, max_length=128, padding=True)\n",
        "        inputs = self.tokenizer.encode_plus(text, add_special_tokens=True, truncation=True, max_length=MaskedLMDataset.MAX_LEN, padding=True)\n",
        "\n",
        "        l = len(inputs[\"input_ids\"])\n",
        "        # This is for padding\n",
        "        input_ids, token_type_ids, attention_mask = pad_data(inputs, pad_token_id=self.tokenizer.pad_token_id, max_length=MaskedLMDataset.MAX_LEN)\n",
        "\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # attention_mask = torch.tensor([[int(token_id != self.tokenizer.pad_token_id) for token_id in inputs[\"input_ids\"]]])\n",
        "        # Mask a token randomly\n",
        "        # mask_idx = torch.randint(1, input_ids.size(1), (1,)).item()\n",
        "\n",
        "        # mask_idx = torch.randint(1, l, (1,)).item()\n",
        "        # input_ids[0, mask_idx] = tokenizer.mask_token_id\n",
        "\n",
        "        labels = input_ids.clone()\n",
        "        # labels[0, mask_idx] = input_ids[0, mask_idx]\n",
        "\n",
        "        return input_ids.squeeze(), attention_mask.squeeze(), labels.squeeze()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "8TYBXYTTNy"
      },
      "source": [
        "# Load pre-trained tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "iowGeO8BDe"
      },
      "source": [
        "# print(model.config.pad_token_id)\n",
        "# print(tokenizer.pad_token_id)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0\n0\n"
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "MevJaDZP0x"
      },
      "source": [
        "# prepare and training\n",
        "dataset = MaskedLMDataset(tokenizer, data[:5])\n",
        "dataloader = DataLoader(dataset, batch_size=30, shuffle=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "nVDcHqK8oG"
      },
      "source": [
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "Td1LfGqLl0"
      },
      "source": [
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    for input_ids, attention_mask, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        # print(labels)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/50, Loss: 4.108638286590576\nEpoch 2/50, Loss: 3.137907028198242\nEpoch 3/50, Loss: 2.447856903076172\nEpoch 4/50, Loss: 1.9152977466583252\nEpoch 5/50, Loss: 1.5211859941482544\nEpoch 6/50, Loss: 1.2373405694961548\nEpoch 7/50, Loss: 1.027313470840454\nEpoch 8/50, Loss: 0.8672327995300293\nEpoch 9/50, Loss: 0.7426521182060242\nEpoch 10/50, Loss: 0.6411861777305603\nEpoch 11/50, Loss: 0.5543447136878967\nEpoch 12/50, Loss: 0.47995203733444214\nEpoch 13/50, Loss: 0.4172589182853699\nEpoch 14/50, Loss: 0.3645837604999542\nEpoch 15/50, Loss: 0.3201811909675598\nEpoch 16/50, Loss: 0.282680481672287\nEpoch 17/50, Loss: 0.25070828199386597\nEpoch 18/50, Loss: 0.22313503921031952\nEpoch 19/50, Loss: 0.19927005469799042\nEpoch 20/50, Loss: 0.17863687872886658\nEpoch 21/50, Loss: 0.1607767641544342\nEpoch 22/50, Loss: 0.1452450007200241\nEpoch 23/50, Loss: 0.13167239725589752\nEpoch 24/50, Loss: 0.11978603899478912\nEpoch 25/50, Loss: 0.1093749850988388\nEpoch 26/50, Loss: 0.10024954378604889\nEpoch 27/50, Loss: 0.09222845733165741\nEpoch 28/50, Loss: 0.08514507114887238\nEpoch 29/50, Loss: 0.07885558903217316\nEpoch 30/50, Loss: 0.07324682921171188\nEpoch 31/50, Loss: 0.0682322308421135\nEpoch 32/50, Loss: 0.06373804807662964\nEpoch 33/50, Loss: 0.059706442058086395\nEpoch 34/50, Loss: 0.05608590692281723\nEpoch 35/50, Loss: 0.05282722786068916\nEpoch 36/50, Loss: 0.04988684505224228\nEpoch 37/50, Loss: 0.047226641327142715\nEpoch 38/50, Loss: 0.04481511563062668\nEpoch 39/50, Loss: 0.04262109845876694\nEpoch 40/50, Loss: 0.040618058294057846\nEpoch 41/50, Loss: 0.038783490657806396\nEpoch 42/50, Loss: 0.03709926828742027\nEpoch 43/50, Loss: 0.03554827719926834\nEpoch 44/50, Loss: 0.034117333590984344\nEpoch 45/50, Loss: 0.03279436379671097\nEpoch 46/50, Loss: 0.03156929835677147\nEpoch 47/50, Loss: 0.03043271228671074\nEpoch 48/50, Loss: 0.02937697432935238\nEpoch 49/50, Loss: 0.0283952709287405\nEpoch 50/50, Loss: 0.027480503544211388\n"
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "wzypa5nPFq"
      },
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "BertForMaskedLM(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (cls): BertOnlyMLMHead(\n    (predictions): BertLMPredictionHead(\n      (transform): BertPredictionHeadTransform(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (transform_act_fn): GELUActivation()\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "Hct8Na7BaL"
      },
      "source": [
        "mask = \"[MASK]\"\n",
        "# # Example text with a masked token\n",
        "text = \"The quick brown dog jumps over the lazy dog.\"\n",
        "\n",
        "\n",
        "MAX_LEN = 128\n",
        "# inputs = self.tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt', truncation=True, max_length=MAX_LEN, padding=True)\n",
        "# inputs = tokenizer.encode_plus(text, add_special_tokens=True, truncation=True, max_length=MAX_LEN, padding=True)\n",
        "\n",
        "# # This is for padding\n",
        "# l = len(inputs[\"input_ids\"])\n",
        "# if l < MAX_LEN:\n",
        "#     inputs[\"input_ids\"]=inputs[\"input_ids\"]+[tokenizer.pad_token_id]*(MAX_LEN-l)\n",
        "# else:\n",
        "#     inputs[\"input_ids\"]=inputs[\"input_ids\"][:MAX_LEN]\n",
        "\n",
        "# input_ids = torch.tensor([inputs[\"input_ids\"]])\n",
        "# # labels = input_ids.clone()\n",
        "# attention_mask = torch.tensor([[int(token_id != tokenizer.pad_token_id) for token_id in inputs[\"input_ids\"]]])\n",
        "\n",
        "# Mask a token randomly\n",
        "# mask_idx = torch.randint(1, input_ids.size(1), (1,)).item()\n",
        "# mask_idx = torch.randint(1, l, (1,)).item()\n",
        "# input_ids[0, mask_idx] = tokenizer.mask_token_id\n",
        "# labels = input_ids.clone()\n",
        "# labels[0, mask_idx] = input_ids[0, mask_idx]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# text = \"I like [MASK] eat apples.\"\n",
        "words = text.split()\n",
        "times = len(words)\n",
        "print(text, times)\n",
        "for i in range(times):\n",
        "    words = text.split()\n",
        "    words[i] = mask\n",
        "    seq = \" \".join(words)\n",
        "    print(seq)\n",
        "    # Tokenize the text\n",
        "    # inputs = tokenizer.encode_plus(seq, return_tensors=\"pt\")\n",
        "    inputs = tokenizer.encode_plus(text, add_special_tokens=True, truncation=True, max_length=MAX_LEN, padding=True)\n",
        "\n",
        "    # This is for padding\n",
        "    l = len(inputs[\"input_ids\"])\n",
        "    if l < MAX_LEN:\n",
        "        inputs[\"input_ids\"]=inputs[\"input_ids\"]+[tokenizer.pad_token_id]*(MAX_LEN-l)\n",
        "    else:\n",
        "        inputs[\"input_ids\"]=inputs[\"input_ids\"][:MAX_LEN]\n",
        "\n",
        "    input_ids = torch.tensor([inputs[\"input_ids\"]])\n",
        "    # labels = input_ids.clone()\n",
        "    attention_mask = torch.tensor([[int(token_id != tokenizer.pad_token_id) for token_id in inputs[\"input_ids\"]]])\n",
        "\n",
        "\n",
        "    # Generate predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids.squeeze(), attention_mask=attention_mask.squeeze())\n",
        "        predictions = outputs.logits.argmax(dim=-1)\n",
        "\n",
        "    # Decode the predicted token\n",
        "    predicted_token = tokenizer.decode(predictions[0, inputs[\"input_ids\"].squeeze().tolist().index(tokenizer.mask_token_id)])\n",
        "\n",
        "    print(\"Predicted token:\", predicted_token)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "The quick brown dog jumps over the lazy dog. 9\n[MASK] quick brown dog jumps over the lazy dog.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\nCell \u001b[0;32mIn[173], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Generate predictions\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 62\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Decode the predicted token\u001b[39;00m\n\nFile \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\nFile \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1335\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1333\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1335\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1350\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n\nFile \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\nFile \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:942\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 942\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[1;32m    943\u001b[0m device \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m inputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# past_key_values_length\u001b[39;00m\n\n\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)\n"
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "6F2YzcQrAV"
      },
      "source": [
        "sentence = \"How much wood would a woodchuck chuck if a [MASK] could chuck wood?\"\n",
        "# inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "\n",
        "inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, truncation=True, max_length=MaskedLMDataset.MAX_LEN, padding=True)\n",
        "\n",
        "input_ids, token_type_ids, attention_mask = pad_data(inputs, pad_token_id=tokenizer.pad_token_id, max_length=MaskedLMDataset.MAX_LEN)\n",
        "\n",
        "mask_idx = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "outputs = model(input_ids.squeeze(),token_type_ids=token_type_ids.squeeze(), attention_mask=attention_mask.squeeze())\n",
        "predictions = outputs.logits[0, mask_idx].argmax(dim=1)\n",
        "predicted_token = tokenizer.decode(predictions)\n",
        "print(\"Predicted token:\", predicted_token)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\nCell \u001b[0;32mIn[86], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m input_ids, token_type_ids, attention_mask \u001b[38;5;241m=\u001b[39m pad_data(inputs, pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id, max_length\u001b[38;5;241m=\u001b[39mMaskedLMDataset\u001b[38;5;241m.\u001b[39mMAX_LEN)\n\u001b[1;32m      8\u001b[0m mask_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(input_ids \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mmask_token_id)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m predictions \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m, mask_idx]\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m predicted_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(predictions)\n\nFile \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\nFile \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1335\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1333\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1335\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1350\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n\nFile \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\nFile \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:942\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 942\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[1;32m    943\u001b[0m device \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m inputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# past_key_values_length\u001b[39;00m\n\n\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)\n"
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "BScz7cYF7U"
      },
      "source": [
        "sentence = \"How much wood would a woodchuck chuck if a [MASK] could chuck wood?\"\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "mask_idx = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "outputs = model(**inputs)\n",
        "predictions = outputs.logits[0, mask_idx].argmax(dim=1)\n",
        "predicted_token = tokenizer.decode(predictions)\n",
        "print(\"Predicted token:\", predicted_token)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Predicted token: [PAD]\n"
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "CJpA9uvMfO"
      },
      "source": [
        "text = \"The [MASK] brown dog jumps over the lazy dog.\"\n",
        "inputs = tokenizer.encode_plus(text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predictions = outputs.logits.argmax(-1)\n",
        "\n",
        "# print(predictions[0].shape)\n",
        "# Decode the predicted token\n",
        "predicted_token = tokenizer.decode(predictions[0, inputs[\"input_ids\"].squeeze().tolist().index(tokenizer.mask_token_id)])\n",
        "\n",
        "print(\"Predicted token:\", predicted_token)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Predicted token: l a z y\n"
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "7tu0jVNbNi"
      },
      "source": [
        "This is for text Generate, text completion and masked token prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "MSy1KAes0U"
      },
      "source": [
        "prompt = \"The quick brown fox\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=100, num_return_sequences=1)\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated text:\", generated_text)\n",
        "\n",
        "\n",
        "text = \"She sells seashells by the seashore. \"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=100, num_return_sequences=1)\n",
        "completed_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Completed text:\", completed_text)\n",
        "\n",
        "sentence = \"How much wood would a woodchuck chuck if a [MASK] could chuck wood?\"\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "mask_idx = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "outputs = model(**inputs)\n",
        "predictions = outputs.logits[0, mask_idx].argmax(dim=1)\n",
        "predicted_token = tokenizer.decode(predictions)\n",
        "print(\"Predicted token:\", predicted_token)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Generated text: the quick brown fox the fox. hunt mr. hunt the fox the fox mr. mr..... the fox the mr...... mr...\nCompleted text: she sells seashells by the seashore.....................................................................................\nPredicted token: man\n"
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "jO6sr5NZGw"
      },
      "source": [
        "# Example of data augmentation\n",
        "# from nlpaug.util import Action\n",
        "# import nlpaug.augmenter.word as naw\n",
        "\n",
        "# # Original data\n",
        "# original_data = [\"The quick brown fox jumps over the lazy dog.\",\n",
        "#                  \"She sells seashells by the seashore.\",\n",
        "#                  \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\"]\n",
        "\n",
        "# # Augmenter for synonym replacement\n",
        "# aug = naw.SynonymAug(aug_src='wordnet')\n",
        "\n",
        "# # Augment data\n",
        "# augmented_data = []\n",
        "# for sentence in original_data:\n",
        "#     augmented_sentence = aug.augment(sentence)\n",
        "#     augmented_data.append(augmented_sentence)\n",
        "\n",
        "# # Print augmented data\n",
        "# for idx, (original, augmented) in enumerate(zip(original_data, augmented_data)):\n",
        "#     print(f\"Original {idx + 1}: {original}\")\n",
        "#     print(f\"Augmented {idx + 1}: {augmented}\")\n",
        "#     print()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Original 1: The quick brown fox jumps over the lazy dog.\nAugmented 1: ['The ready john brown fox jumps ended the lazy dog.']\n\nOriginal 2: She sells seashells by the seashore.\nAugmented 2: ['She sell seashells by the coast.']\n\nOriginal 3: How much wood would a woodchuck chuck if a woodchuck could chuck wood?\nAugmented 3: ['How much wood would a woodchuck chuck if a marmota monax could regorge wood?']\n\n"
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "OSQQuRaavF"
      },
      "source": [],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Predicted token: t o\n"
        }
      ],
      "execution_count": 11
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}