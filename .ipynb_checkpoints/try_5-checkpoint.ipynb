{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "ajvEl0eORu"
      },
      "source": [],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "lCrdzqv4ot"
      },
      "source": [
        "import torch\n",
        "from utils import  JSONDataset, RevdictModel, PAD, EOS, BOS, UNK, AraT5RevDict\n",
        "import tqdm # progree bar"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "gHn2qwYPcp"
      },
      "source": [
        "# Step 1: Prepare the Dataset\n",
        "dataset_file = 'dev.json'  # Replace with your dataset file path\n",
        "dataset = JSONDataset(dataset_file)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "Zis7H5ioFE"
      },
      "source": [
        "print(dataset.tensors)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "P4KvGPgiky"
      },
      "source": [
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset) - 100, 100])\n",
        "# print(dataset.vocab)\n",
        "# Step 2: Model Selection\n",
        "model = RevdictModel(dataset.vocab, d_model=256, n_head=4, n_layers=4, dropout=0.3, maxlen=256)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "tX5iXzQ4mF"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The RevdictModel model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "2AusxkiuAX"
      },
      "source": [
        "# Hayper\n",
        "EPOCHS, LEARNING_RATE, BETA1, BETA2, WEIGHT_DECAY = 10, 1.0e-4, 0.9, 0.999, 1.0e-6\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=LEARNING_RATE,\n",
        "        betas=(BETA1, BETA2),\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "loss_fn = torch.nn.MSELoss()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "sHZFz8H6D8"
      },
      "source": [
        "# Step 3: Model Training\n",
        "for epoch in tqdm.trange(EPOCHS, desc=\"Epoch\"):  \n",
        "    model.train()\n",
        "    for batch in train_set:\n",
        "\n",
        "        gloss_tensor = batch['gloss_tensor'].unsqueeze(1)\n",
        "        gloss_emb = batch['electra_tensor'].unsqueeze(1)  # Choose one of the embeddings (bertseg, bertmsa, electra)\n",
        "        # print(gloss_emb.size(), gloss_tensor.size())\n",
        "        # break\n",
        "        optimizer.zero_grad()\n",
        "        output_emb = model(gloss_tensor)\n",
        "        loss = loss_fn(output_emb, gloss_emb)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "WTrPutRJhI"
      },
      "source": [
        "# Step 4: Model Evaluation\n",
        "model.eval()\n",
        "total_loss = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_set:\n",
        "        gloss_tensor = batch['gloss_tensor'].unsqueeze(1)\n",
        "        gloss_emb = batch['electra_tensor'].unsqueeze(1)\n",
        "        # print(gloss_tensor)\n",
        "        output_emb = model(gloss_tensor)\n",
        "        loss = loss_fn(output_emb, gloss_emb)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "avg_loss = total_loss / len(test_set)\n",
        "print(f\"Average Test Loss: {avg_loss:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "7E3jnBnbkj"
      },
      "source": [
        "# embedding_list=[data[\"electra_tensor\"] for data in dataset]\n",
        "# print(embedding_list[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "PBHwitAMbT"
      },
      "source": [
        "# Assuming 'model' is your trained model\n",
        "model.vocab = dataset.vocab\n",
        "# dataset.tensors = embedding_list\n",
        "\n",
        "freeze_vocab=0\n",
        "maxlen = 256\n",
        "# Step 5: Reverse Dictionary Lookup\n",
        "def find_similar_words(gloss, n=5):\n",
        "    with torch.no_grad():\n",
        "        # if gloss not in model.vocab:\n",
        "        #     print(f\"Gloss '{gloss}' is not in the vocabulary.\")\n",
        "            # return []\n",
        "        # gloss_tensor = torch.tensor(model.vocab[gloss])\n",
        "        # gloss_tensor = torch.clamp(gloss_tensor, min=0, max=model.embedding.num_embeddings - 1)\n",
        "        # gloss_tensor = model.embedding(gloss_tensor)\n",
        "        gloss_tensor= torch.tensor(\n",
        "                [model.vocab[PAD]] + [\n",
        "                    model.vocab[word]\n",
        "                    if not freeze_vocab\n",
        "                    else dataset.vocab.get(word, model.vocab[UNK])\n",
        "                    for word in gloss.split()\n",
        "                ]\n",
        "                + [model.vocab[EOS]])\n",
        "\n",
        "        if maxlen:\n",
        "            gloss_tensor = gloss_tensor[:maxlen]\n",
        "\n",
        "        gloss_emb = model(gloss_tensor.unsqueeze(1))\n",
        "        distances = torch.cdist(gloss_emb, dataset.tensors, p=2)\n",
        "        closest_indices = torch.topk(distances, n, largest=False).indices\n",
        "        closest_words = [dataset.itos[idx] for idx in closest_indices.squeeze()]\n",
        "\n",
        "        # print(\"Size of the vocabulary:\", len(model.vocab))\n",
        "        # print(\"Index being passed to embedding layer:\", model.vocab[gloss])\n",
        "        return closest_words\n",
        "\n",
        "query_gloss = \"\u063a\u064e\u0645\u0651\u064e \u0648\u064e\u0623\u064e\u0645\u0652\u0631\u064e\u0636\u064e \u0627\u0644\u0642\u064e\u0644\u0652\u0628\u064e\"  # Replace with your query gloss\n",
        "# query_gloss = dev_df.loc[0,\"gloss\"]\n",
        "similar_words = find_similar_words(query_gloss)\n",
        "if similar_words:\n",
        "    print(f\"Similar words for '{query_gloss}': {similar_words}\")\n",
        "else:\n",
        "    print(f\"No similar words found for '{query_gloss}'.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "6KgAJ7b47Q"
      },
      "source": [
        "import pandas as pd\n",
        "dev_df=pd.read_json(\"dev.json\")\n",
        "print(dev_df.info())\n",
        "print(dev_df.loc[1,\"gloss\"])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "6DlzWGpHqA"
      },
      "source": [
        "# Assuming 'model' is your trained model\n",
        "model.vocab = dataset.vocab\n",
        "dataset.tensors = embedding_list\n",
        "\n",
        "freeze_vocab=0\n",
        "maxlen = 256\n",
        "# Step 5: Reverse Dictionary Lookup\n",
        "def find_similar_words(gloss, n=5):\n",
        "    with torch.no_grad():\n",
        "        # if gloss not in model.vocab:\n",
        "        #     print(f\"Gloss '{gloss}' is not in the vocabulary.\")\n",
        "            # return []\n",
        "        # gloss_tensor = torch.tensor(model.vocab[gloss])\n",
        "        # gloss_tensor = torch.clamp(gloss_tensor, min=0, max=model.embedding.num_embeddings - 1)\n",
        "        # gloss_tensor = model.embedding(gloss_tensor)\n",
        "        gloss_tensor= torch.tensor(\n",
        "                [model.vocab[PAD]] + [\n",
        "                    model.vocab[word]\n",
        "                    if not freeze_vocab\n",
        "                    else dataset.vocab.get(word, model.vocab[UNK])\n",
        "                    for word in gloss.split()\n",
        "                ]\n",
        "                + [model.vocab[EOS]])\n",
        "\n",
        "        if maxlen:\n",
        "            gloss_tensor = gloss_tensor[:maxlen]\n",
        "\n",
        "        gloss_emb = model(gloss_tensor.unsqueeze(1))\n",
        "        dataset_tensor = dataset.tensors[0].unsqueeze(1)\n",
        "        distances = torch.cdist(gloss_emb, dataset_tensor, p=2)\n",
        "        closest_indices = torch.topk(distances, n, largest=False).indices\n",
        "        closest_words = [dataset.itos[idx] for idx in closest_indices.squeeze()]\n",
        "\n",
        "        # print(\"Size of the vocabulary:\", len(model.vocab))\n",
        "        # print(\"Index being passed to embedding layer:\", model.vocab[gloss])\n",
        "        return closest_words\n",
        "\n",
        "# query_gloss = \"\u063a\u064e\u0645\u064e\u0651 \u0648\u064e\u0623\u064e\u0645\u0652\u0631\u064e\u0636\u064e \u0627\u0644\u0642\u064e\u0644\u0652\u0628\u064e\"  # Replace with your query gloss\n",
        "query_gloss = dev_df.loc[1,\"gloss\"]\n",
        "similar_words = find_similar_words(query_gloss)\n",
        "if similar_words:\n",
        "    print(f\"Similar words for '{query_gloss}': {similar_words}\")\n",
        "else:\n",
        "    print(f\"No similar words found for '{query_gloss}'.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "TmPwvv6WKI"
      },
      "source": [
        "# [*map(lambda x:x[0], embeddings_df.values)]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "IY2v6WXWVi"
      },
      "source": [
        "import pandas as pd\n",
        "dev_df = pd.read_json(\"dev.json\")\n",
        "\n",
        "print(dev_df.info())\n",
        "print(dev_df.set_index(\"word\"))\n",
        "# print(dev_df.loc[0,[\"word\", \"gloss\"]])\n",
        "# dev_df.loc[0,\"electra\"]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "2gfWkKiHD0"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the word embeddings (assuming they are in a DataFrame)\n",
        "# embeddings_df = pd.read_json(\"dev.json\")\n",
        "embeddings_df = pd.read_json(\"train.json\")\n",
        "\n",
        "# print(embeddings_df.columns)\n",
        "# Drop unnecessary columns and set the word as the index\n",
        "embeddings_df = embeddings_df.set_index(\"word\").drop([\"id\", \"pos\", \"gloss\", \"bertseg\", \"bertmsa\"], axis=1)\n",
        "\n",
        "# print(embeddings_df.dtypes)\n",
        "# print(embeddings_df.iloc[0,0])\n",
        "# print()\n",
        "x = [*map(lambda x:x[0], embeddings_df.values)]\n",
        "len(x)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "gToCMHqVWi"
      },
      "source": [
        "# # Perform PCA to reduce dimensionality to 2 dimensions\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform([*map(lambda x:x[0], embeddings_df.values)])\n",
        "\n",
        "# # Create a new DataFrame with the reduced embeddings\n",
        "reduced_df = pd.DataFrame(reduced_embeddings, index=embeddings_df.index, columns=[\"x\", \"y\"])\n",
        "\n",
        "# Plot the word embeddings using a scatter plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(reduced_df[\"x\"], reduced_df[\"y\"])\n",
        "\n",
        "# # Add labels for the words (you can adjust the font size as needed)\n",
        "for word, pos in reduced_df.iterrows():\n",
        "    plt.text(pos.x, pos.y, word, fontsize=8)\n",
        "\n",
        " # plt.xlabel(\"PCA Dimension 1\")\n",
        " # plt.ylabel(\"PCA Dimension 2\")\n",
        " # plt.title(\"Word Embedding Visualization using PCA\")\n",
        " # plt.grid(True)\n",
        " # plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "oHtTw2IvrO"
      },
      "source": [
        "# revdict5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "Qzfs6cOoQB"
      },
      "source": [
        "import argparse\n",
        "import itertools\n",
        "import json\n",
        "import logging\n",
        "import pathlib\n",
        "import sys\n",
        "\n",
        "from utils import  JSONDataset, RevdictModel, PAD, EOS, BOS, UNK, AraT5RevDict\n",
        "\n",
        "logger = logging.getLogger(pathlib.Path(\"x.log\").name)\n",
        "logger.setLevel(logging.DEBUG)\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "handler.setFormatter(\n",
        "    logging.Formatter(\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\n",
        ")\n",
        "logger.addHandler(handler)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import tqdm\n",
        "\n",
        "# import data\n",
        "# import models\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "# from ard_dataset import "
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "8GYYxcnxec"
      },
      "source": [
        "def read_json(path):\n",
        "    with open(path, 'r', encoding='utf-8') as fin:\n",
        "        data = json.load(fin)\n",
        "    return data\n",
        "\n",
        "def write_json(path, data):\n",
        "    with open(path, 'w', encoding='utf-8') as fout:\n",
        "        json.dump(data, fout)\n",
        "\n",
        "\n",
        "class ARDDataset(Dataset):\n",
        "    def __init__(self, path, is_test=False) -> None:\n",
        "        super().__init__()\n",
        "        self.is_test = is_test\n",
        "        self.data = read_json(path)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.data[index]\n",
        "        if self.is_test:\n",
        "            return sample[\"id\"], sample[\"word\"], sample[\"gloss\"],\n",
        "        else:\n",
        "            return sample[\"id\"], sample[\"word\"], sample[\"gloss\"], sample[\"electra\"], sample[\"bertseg\"], sample['bertmsa']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "gZW4QdCGmK"
      },
      "source": [
        "def rank_cosine(preds, targets):\n",
        "    assocs = F.normalize(preds) @ F.normalize(targets).T\n",
        "    refs = torch.diagonal(assocs, 0).unsqueeze(1)\n",
        "    ranks = (assocs >= refs).sum(1).float()\n",
        "    assert ranks.numel() == preds.size(0)\n",
        "    ranks = ranks.mean().item()\n",
        "    return ranks / preds.size(0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "kCSTM0MjE8"
      },
      "source": [
        "def train(args):\n",
        "    assert args.train_file is not None, \"Missing dataset for training\"\n",
        "    # 1. get data, vocabulary, summary writer\n",
        "    logger.debug(\"Preloading data\")\n",
        "    ## make datasets\n",
        "    train_dataset = ARDDataset(args.train_file)\n",
        "    valid_dataset = ARDDataset(args.dev_file)\n",
        "    \n",
        "    ## make dataloader\n",
        "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=args.batch_size)\n",
        "    valid_dataloader = DataLoader(valid_dataset, batch_size=args.batch_size)\n",
        "    ## make summary writer\n",
        "    summary_writer = SummaryWriter(args.save_dir / args.summary_logdir)\n",
        "    train_step = itertools.count()  # to keep track of the training steps for logging\n",
        "\n",
        "    # 2. construct model\n",
        "    ## Hyperparams\n",
        "    logger.debug(\"Setting up training environment\")\n",
        "\n",
        "    model = AraT5RevDict(args).to(args.device)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"UBC-NLP/AraT5v2-base-1024\")     \n",
        "    model.train()\n",
        "\n",
        "    # 3. declare optimizer & loss_fn\n",
        "    ## Hyperparams\n",
        "    EPOCHS, LEARNING_RATE, BETA1, BETA2, WEIGHT_DECAY = 20, 1.0e-4, 0.9, 0.999, 1.0e-6\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=LEARNING_RATE,\n",
        "        betas=(BETA1, BETA2),\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "    )\n",
        "\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    vec_tensor_key = f\"{args.target_arch}_tensor\"\n",
        "\n",
        "    best_cosine = 0\n",
        "\n",
        "    # 4. train model\n",
        "    for epoch in tqdm.trange(EPOCHS, desc=\"Epochs\"):\n",
        "        ## train loop\n",
        "        pbar = tqdm.tqdm(\n",
        "            desc=f\"Train {epoch}\", total=len(train_dataset), disable=None, leave=False\n",
        "        )\n",
        "        for ids, word, gloss, electra, bertseg, bertmsa in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            word_tokens = tokenizer(word, padding=True, return_tensors='pt').to(args.device)\n",
        "            gloss_tokens = tokenizer(gloss, padding=True, return_tensors='pt').to(args.device)\n",
        "\n",
        "            if args.target_arch == \"electra\":\n",
        "                target_embs = torch.stack(electra, dim=1).to(args.device)\n",
        "            elif args.target_arch ==\"bertseg\":\n",
        "                target_embs = torch.stack(bertseg, dim=1).to(args.device)\n",
        "            elif args.target_arch ==\"bertmsa\":\n",
        "                target_embs = torch.stack(bertmsa, dim=1).to(args.device)\n",
        "\n",
        "\n",
        "            target_embs = target_embs.float()\n",
        "            ce_loss, pred_embs = model(\n",
        "                gloss_tokens[\"input_ids\"], \n",
        "                gloss_tokens[\"attention_mask\"],\n",
        "                word_tokens[\"input_ids\"],\n",
        "            )\n",
        "\n",
        "            mse_loss = loss_fn(pred_embs, target_embs)\n",
        "            loss = args.ce_loss_weight * ce_loss + mse_loss\n",
        "            loss.backward()\n",
        "\n",
        "            # keep track of the train loss for this step\n",
        "            next_step = next(train_step)\n",
        "            summary_writer.add_scalar(\n",
        "                \"revdict-train/cos\",\n",
        "                F.cosine_similarity(pred_embs, target_embs).mean().item(),\n",
        "                next_step,\n",
        "            )\n",
        "            summary_writer.add_scalar(\"revdict-train/mse\", mse_loss.item(), next_step)\n",
        "            optimizer.step()\n",
        "            pbar.update(target_embs.size(0))\n",
        "\n",
        "        pbar.close()\n",
        "        ## eval loop\n",
        "        if args.dev_file:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                sum_dev_loss, sum_cosine, sum_rnk = 0.0, 0.0, 0.0\n",
        "                pbar = tqdm.tqdm(\n",
        "                    desc=f\"Eval {epoch}\",\n",
        "                    total=len(valid_dataset),\n",
        "                    disable=None,\n",
        "                    leave=False,\n",
        "                )\n",
        "                pred_embs_list, target_embs_list = [], []\n",
        "                for ids, word, gloss, electra, bertseg, bertmsa in valid_dataloader:\n",
        "                    word_tokens = tokenizer(word, padding=True, return_tensors='pt').to(args.device)\n",
        "                    gloss_tokens = tokenizer(gloss, padding=True, return_tensors='pt').to(args.device)\n",
        "                    \n",
        "                    # word_tokens = tokenizer(word, padding=True, return_tensors='pt').to(args.device)\n",
        "                    # gloss_tokens = tokenizer(gloss, max_length=512, padding=True, truncation=True, return_tensors='pt').to(args.device)\n",
        "\n",
        "                    if args.target_arch == \"electra\":\n",
        "                        target_embs = torch.stack(electra, dim=1).to(args.device)\n",
        "                    elif args.target_arch == \"bertseg\":\n",
        "                        target_embs = torch.stack(bertseg, dim=1).to(args.device)\n",
        "                    elif args.target_arch == \"bertmsa\":\n",
        "                        target_embs = torch.stack(bertmsa, dim=1).to(args.device)\n",
        "\n",
        "                    target_embs = target_embs.float()\n",
        "                    ce_loss, pred_embs = model(\n",
        "                        gloss_tokens[\"input_ids\"], \n",
        "                        gloss_tokens[\"attention_mask\"],\n",
        "                        word_tokens[\"input_ids\"],\n",
        "                    )\n",
        "\n",
        "                    mse_loss = loss_fn(pred_embs, target_embs)\n",
        "                    loss = args.ce_loss_weight * ce_loss + mse_loss\n",
        "\n",
        "                    sum_dev_loss += loss.item()\n",
        "                    sum_cosine += F.cosine_similarity(pred_embs, target_embs).sum().item()\n",
        "                    pred_embs_list.append(pred_embs.cpu())\n",
        "                    target_embs_list.append(target_embs.cpu())\n",
        "                    # sum_rnk += rank_cosine(pred_embs, target_embs)\n",
        "                    pbar.update(target_embs.size(0))\n",
        "                \n",
        "                sum_rnk = rank_cosine(torch.cat(pred_embs_list, dim=0), torch.cat(target_embs_list, dim=0))\n",
        "                pbar = tqdm.tqdm(\n",
        "                    desc=f\"Eval {epoch} cos: \"+str(sum_cosine / len(valid_dataset))+\" mse: \"+str( sum_dev_loss / len(valid_dataset) )+\" rnk: \"+str(sum_rnk/ len(valid_dataset))+ \" sum_rnk: \"+str(sum_rnk)+\" len of dev: \"+str(len(valid_dataset)) +\"\\n\",\n",
        "                    total=len(valid_dataset),\n",
        "                    disable=None,\n",
        "                    leave=False,\n",
        "                )\n",
        "\n",
        "                if sum_cosine >= best_cosine:\n",
        "                    best_cosine = sum_cosine\n",
        "                    print(f\"Saving Best Checkpoint at Epoch {epoch}\")\n",
        "                    model.save(args.save_dir / \"model_best.pt\")\n",
        "\n",
        "                # keep track of the average loss on dev set for this epoch\n",
        "                summary_writer.add_scalar(\n",
        "                    \"revdict-dev/cos\", sum_cosine / len(valid_dataset), epoch\n",
        "                )\n",
        "                summary_writer.add_scalar(\n",
        "                    \"revdict-dev/mse\", sum_dev_loss / len(valid_dataset), epoch\n",
        "                )\n",
        "                summary_writer.add_scalar(\n",
        "                    \"revdict-dev/rnk\", sum_rnk / len(valid_dataset), epoch\n",
        "                )\n",
        "                pbar.close()\n",
        "                model.train()\n",
        "\n",
        "        model.save(args.save_dir / \"modelepoch.pt\")\n",
        "            \n",
        "    # 5. save result\n",
        "    model.save(args.save_dir / \"model.pt\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "aSmKrELSlX"
      },
      "source": [
        "class Args(object):\n",
        "  def __init__(self, **kwargs):\n",
        "    self.__dict__ = kwargs\n",
        "\n",
        "# python revdict.py --do_train --train_file ../../../dev.json --dev_file ../../../dev.json  --model_name \"aubmindlab/bert-base-arabertv02\"\n",
        "\n",
        "args_train = Args(\n",
        "    # train_file = pathlib.Path(\"train.json\"),\n",
        "    train_file = pathlib.Path(\"dev.json\"),\n",
        "    # dev_file = pathlib.Path(\"dev.json\"),\n",
        "    save_dir = pathlib.Path(\"models/\"),\n",
        "    summary_logdir =pathlib.Path(\"logs/\"),\n",
        "    model_name=\"aubmindlab/bert-base-arabertv02\",\n",
        "    # device=\"cuda\",\n",
        "    device=\"cpu\",\n",
        "    target_arch=\"electra\", # choices=(\"sgns\", \"electra\", \"bertseg\", \"bertmsa\"),\n",
        "\n",
        "    batch_size=64,\n",
        "    resume_train=None,#\"/content/drive/MyDrive/data_for_KKSA_NLP_CH/models/modelepoch.pt\",\n",
        "    resume_file=None,\n",
        "    from_pretrained=False,\n",
        "    max_len=256, # choices=(300, 256, 768),\n",
        "    num_epochs=10\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "gn451sUfOx"
      },
      "source": [
        "train(args_train)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "5kLNgpkrsN"
      },
      "source": [
        "import pandas as pd\n",
        "x =  pd.read_json(\"dev.json\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "F1xrXeXHCW"
      },
      "source": [
        "print(len(x.loc[0,\"bertmsa\"]))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "d9qVMQEpnG"
      },
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}